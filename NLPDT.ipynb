{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Initialize the accelerator\naccelerator = Accelerator()\n\n# read data\ntrain_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Package Importing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, RobertaTokenizer, RobertaModel\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader\nfrom accelerate import Accelerator\nfrom tqdm import tqdm\nimport random\nimport copy\nimport os\nimport multiprocessing\nfrom sklearn.model_selection import StratifiedKFold\nimport string\nimport re\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom collections import OrderedDict","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-07-07T02:32:57.557413Z","iopub.execute_input":"2023-07-07T02:32:57.557874Z","iopub.status.idle":"2023-07-07T02:32:57.566346Z","shell.execute_reply.started":"2023-07-07T02:32:57.557838Z","shell.execute_reply":"2023-07-07T02:32:57.565460Z"}}},{"cell_type":"code","source":"# remove duplicates\ntrain_no_duplicates = train_raw.drop_duplicates('text')\n# remove contradictory\nduplicates_df = train_raw[train_raw.text.duplicated(keep=False)].sort_values('text')\ncontradictory_tweets = set()\nfor tweet in list(duplicates_df.text):\n    if len(set(duplicates_df[duplicates_df['text'] == tweet].target)) > 1:\n        contradictory_tweets.add(tweet)\n\ncontradictory_tweets = list(contradictory_tweets)\n\nfiltered_df = train_no_duplicates[~train_no_duplicates['text'].isin(contradictory_tweets)]\n\n# Text cleaning function\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    # Remove user @ references and '#' from text\n    text = re.sub(r'\\@\\w+|\\#','', text)\n    # Remove punctuations\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Convert to lowercase to maintain consistency\n    text = text.lower()\n    return text\n\n# Apply the text cleaning function\nfiltered_df['clean_text'] = filtered_df['text'].apply(clean_text)\ntest['clean_text'] = test['text'].apply(clean_text)\n\ndef combine_columns(row):\n    values = [f\"{col}: {str(row[col])}\" for col in row.index[1:-2] if pd.notnull(row[col])]\n    return ' '.join(values)\n\n# Combine the three columns into a single column\nfiltered_df['combined'] = filtered_df.apply(combine_columns, axis=1)\nfiltered_df['final_text'] =  filtered_df['clean_text']+' '+ filtered_df['combined']\n\n#test set\nfiltered_test = test.copy()\nfiltered_test['combined'] = filtered_test.apply(combine_columns, axis=1)\nfiltered_test['final_text'] =  filtered_test['clean_text']+' '+ filtered_test['combined']","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:44:54.648331Z","iopub.execute_input":"2023-05-22T21:44:54.648762Z","iopub.status.idle":"2023-05-22T21:45:09.011742Z","shell.execute_reply.started":"2023-05-22T21:44:54.648731Z","shell.execute_reply":"2023-05-22T21:45:09.009856Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"18\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/2659865553.py:74: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['clean_text'] = filtered_df['text'].apply(clean_text)\n/tmp/ipykernel_32/2659865553.py:82: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['combined'] = filtered_df.apply(combine_columns, axis=1)\n/tmp/ipykernel_32/2659865553.py:83: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['final_text'] =  filtered_df['clean_text']+' '+ filtered_df['combined']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT Model","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def train_valid_split(self, train_fraction=.8, shuffle=True):\n        num_train_examples = int(len(self) * train_fraction)\n        train_dataset = copy.deepcopy(self)\n        \n        if shuffle:\n            random.shuffle(train_dataset.data)\n        \n        valid_dataset = copy.deepcopy(train_dataset)\n        train_dataset.data = train_dataset.data[:num_train_examples]\n        valid_dataset.data = valid_dataset.data[num_train_examples:]\n        \n        return train_dataset, valid_dataset","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n        \n        self.model = AutoModel.from_pretrained(model)\n        hidden_size = self.model.config.hidden_size\n        \n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, classes) # projection\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        res = self.model.forward(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n        res = res[0]\n        res = res[:,0,:] # encoding for <s> token\n        res = self.project(res)\n        return res\n    \n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          accelerator,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_best_BERT.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n                y_true = accelerator.prepare(y_true)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        y_true = accelerator.prepare(y_true)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n        loss = loss_fn(y_hat, hots)\n        \n        # backprop\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k += 1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        state_dict = torch.load(filepath)\n\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if \"module.\" not in k:\n                name = 'module.' + k\n                new_state_dict[name] = v\n\n        model.load_state_dict(new_state_dict)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\nmodel_checkpoint = \"bert-large-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Create an instance of your custom Dataset class\ndataset = MyDataset(filtered_df, 'final_text', tokenizer)\ntrain_dataset, valid_dataset = dataset.train_valid_split()\n\ndataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)\n\n# Create a DataLoader\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\nmodel = FinetuneClassifier(head_dropout=.1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \nlogs_dict = {}","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:45:42.722106Z","iopub.execute_input":"2023-05-22T21:45:42.722606Z","iopub.status.idle":"2023-05-22T21:50:42.360378Z","shell.execute_reply.started":"2023-05-22T21:45:42.722567Z","shell.execute_reply":"2023-05-22T21:50:42.359411Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train(\n  model, \n  train_dataloader, \n  valid_dataloader, \n  2000, \n  optimizer, \n  accelerator,\n  blind_steps=100, \n  additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n  losses_dict=logs_dict,\n  scheduler=scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T21:45:42.722106Z","iopub.execute_input":"2023-05-22T21:45:42.722606Z","iopub.status.idle":"2023-05-22T21:50:42.360378Z","shell.execute_reply.started":"2023-05-22T21:45:42.722567Z","shell.execute_reply":"2023-05-22T21:50:42.359411Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86f5ece25dda482cbdea6549c021c4a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2604e961b7d840a6bf7ac676c0a0dba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e1537812b84d3ab111dea11d009409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd286860e4cd4b04ac17f54ea7e83194"}},"metadata":{}},{"name":"stderr","text":"100%|████████████████████████████| 7485/7485 [00:12<00:00, 583.51it/s]\n100%|███████████████████████████| 3263/3263 [00:02<00:00, 1165.81it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef68a3f8ec0d4c8f9bf72d9a44426657"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.281 | precision: 0.398 | recall: 0.217 | accuracy: 0.525 | valid_loss: 0.691 ","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [03:19<00:00,  1.99s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.722 | precision: 0.784 | recall: 0.669 | accuracy: 0.780 | valid_loss: 0.489 | train_loss: 0.606 | best_score: 0.722\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RoBERTa Model","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def train_valid_split(self, train_fraction=.8, shuffle=True):\n        num_train_examples = int(len(self) * train_fraction)\n        train_dataset = copy.deepcopy(self)\n        \n        if shuffle:\n            random.shuffle(train_dataset.data)\n        \n        valid_dataset = copy.deepcopy(train_dataset)\n        train_dataset.data = train_dataset.data[:num_train_examples]\n        valid_dataset.data = valid_dataset.data[num_train_examples:]\n        \n        return train_dataset, valid_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-07T02:30:19.413044Z","iopub.execute_input":"2023-07-07T02:30:19.413775Z","iopub.status.idle":"2023-07-07T02:30:19.866301Z","shell.execute_reply.started":"2023-07-07T02:30:19.413737Z","shell.execute_reply":"2023-07-07T02:30:19.864646Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataframe, text_column, tokenizer, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"],"ename":"NameError","evalue":"name 'Dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n        \n        self.model = RobertaModel.from_pretrained(model)\n        hidden_size = self.model.config.hidden_size\n        \n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, classes) # projection\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        res = self.model.forward(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n        res = res[0]\n        res = res[:,0,:] # encoding for <s> token\n        res = self.project(res)\n        return res\n    \n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          accelerator,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_best_RoBERTa.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n                y_true = accelerator.prepare(y_true)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        y_true = accelerator.prepare(y_true)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n        loss = loss_fn(y_hat, hots)\n        \n        # backprop\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k += 1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        state_dict = torch.load(filepath)\n\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if \"module.\" not in k:\n                name = 'module.' + k\n                new_state_dict[name] = v\n\n        model.load_state_dict(new_state_dict)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\nmodel_checkpoint = \"roberta-large\"\n\ntokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\nmodel = RobertaModel.from_pretrained(model_checkpoint)\n\n# Create an instance of your custom Dataset class\ndataset = MyDataset(filtered_df, 'final_text', tokenizer)\ntrain_dataset, valid_dataset = dataset.train_valid_split()\n\ndataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)\n\n# Create a DataLoader\nbatch_size = 16\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\nmodel = FinetuneClassifier(head_dropout=.1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-6, weight_decay=1.5e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \nlogs_dict = {}","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:05:44.569876Z","iopub.execute_input":"2023-05-22T22:05:44.570253Z","iopub.status.idle":"2023-05-22T22:06:01.718868Z","shell.execute_reply.started":"2023-05-22T22:05:44.570221Z","shell.execute_reply":"2023-05-22T22:06:01.717712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train(\n  model, \n  train_dataloader, \n  valid_dataloader, \n  2000, \n  optimizer, \n  accelerator,\n  blind_steps=100, \n  additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n  losses_dict=logs_dict,\n  scheduler=scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:05:44.569876Z","iopub.execute_input":"2023-05-22T22:05:44.570253Z","iopub.status.idle":"2023-05-22T22:06:01.718868Z","shell.execute_reply.started":"2023-05-22T22:05:44.570221Z","shell.execute_reply":"2023-05-22T22:06:01.717712Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# RoBERTa Model with K-Fold","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n        \n        self.model = RobertaModel.from_pretrained(model)\n        hidden_size = self.model.config.hidden_size\n        \n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, classes)\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        res = self.model.forward(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n        res = res[0]\n        res = res[:,0,:] # encoding for <s> token\n        res = self.project(res)\n        return res\n    \n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          accelerator,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_best_RoBERTa.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n                y_true = accelerator.prepare(y_true)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        y_true = accelerator.prepare(y_true)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n        loss = loss_fn(y_hat, hots)\n        \n        # backprop\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k += 1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        state_dict = torch.load(filepath)\n\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if \"module.\" not in k:\n                name = 'module.' + k\n                new_state_dict[name] = v\n\n        model.load_state_dict(new_state_dict)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\nmodel_checkpoint = \"roberta-large\"\n\ntokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\nmodel = RobertaModel.from_pretrained(model_checkpoint)\n\n# Define the batch size and the number of folds\nbatch_size = 16\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds)\n\n# Iterating over each fold\nfor fold, (train_index, valid_index) in enumerate(skf.split(filtered_df, filtered_df['target'])):\n\n    print(f'FOLD {fold + 1}')\n    \n    # Split the data into train and validation datasets for the current fold\n    train_dataset = MyDataset(filtered_df.iloc[train_index], 'final_text', tokenizer)\n    valid_dataset = MyDataset(filtered_df.iloc[valid_index], 'final_text', tokenizer)\n    \n    # Create DataLoaders for the current fold\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Initialize model for the current fold\n    model = FinetuneClassifier(head_dropout=.1)\n    model = nn.DataParallel(model)\n    model.to(device)\n    \n    # Initialize optimizer for the current fold\n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-6, weight_decay=1e-3)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \n    logs_dict = {}\n    \n    # Train the model for the current fold\n    train(\n      model, \n      train_dataloader, \n      valid_dataloader, \n      1500, \n      optimizer, \n      accelerator,\n      blind_steps=100, \n      additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n      filepath=f'model_best_RoBERTa_fold_{fold}.pt',\n      losses_dict=logs_dict,\n      scheduler=scheduler\n    )\n\ndataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)\n\nfor fold in range(n_folds):\n    model_path = f'/kaggle/working/model_best_RoBERTa_fold_{fold}.pt'\n    state_dict = torch.load(model_path)\n\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        if 'module.' not in k:\n            name = 'module.' + k\n            new_state_dict[name] = v\n\n    model.load_state_dict(new_state_dict)\n    \n    predictions_df = pd.DataFrame()\n    for i, (ids, mask) in tqdm(dataset_test):\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        pred = model(input_ids=ids[None], attention_mask=mask[None])[0]\n        y_hat = 1 if pred[0] < pred[1] else 0\n        r = [int(i), y_hat]\n        predictions_df = pd.concat([predictions_df, pd.DataFrame(np.array(r)[None,:], columns=['id', 'target'])])\n    \n    # Add the fold's predictions to the combined predictions DataFrame\n    predictions_df.columns = ['id', f'target_fold_{fold}']\n    if fold == 0:\n        combined_predictions = predictions_df\n    else:\n        combined_predictions = combined_predictions.merge(predictions_df, on='id')\n\ncombined_predictions['target'] = combined_predictions.iloc[:, 1:].mean(axis=1).round().astype(int)\ncombined_predictions[['id', 'target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-03T22:06:36.266535Z","iopub.execute_input":"2023-06-03T22:06:36.268672Z","iopub.status.idle":"2023-06-04T02:54:41.101161Z","shell.execute_reply.started":"2023-06-03T22:06:36.268643Z","shell.execute_reply":"2023-06-04T02:54:41.100081Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/tmp/ipykernel_31/3457697087.py:72: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['clean_text'] = filtered_df['text'].apply(clean_text)\n/tmp/ipykernel_31/3457697087.py:80: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['combined'] = filtered_df.apply(combine_columns, axis=1)\n/tmp/ipykernel_31/3457697087.py:81: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['final_text'] =  filtered_df['clean_text']+' '+ filtered_df['combined']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc4ce31266545ea8fdfb4295b12d786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdee624ad91147079adb107d75e8c629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d5a5ade61e4050b885590d26017881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15738783aff544fdb77d3473b5faa10b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"FOLD 1\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████| 5988/5988 [00:14<00:00, 417.96it/s]\n100%|████████████████████████████| 1497/1497 [00:01<00:00, 787.64it/s]\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.598 | precision: 0.426 | recall: 1.000 | accuracy: 0.426 | valid_loss: 0.782 ","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 100/1500 [03:23<6:26:13, 16.55s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.539 | precision: 0.914 | recall: 0.382 | accuracy: 0.721 | valid_loss: 0.593 | train_loss: 0.701 | best_score: 0.598\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 200/1500 [06:51<6:18:06, 17.45s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.781 | precision: 0.882 | recall: 0.701 | accuracy: 0.832 | valid_loss: 0.408 | train_loss: 0.491 | best_score: 0.781\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 300/1500 [10:20<5:47:06, 17.36s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.783 | precision: 0.851 | recall: 0.726 | accuracy: 0.829 | valid_loss: 0.385 | train_loss: 0.430 | best_score: 0.783\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 400/1500 [13:47<5:19:17, 17.42s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.804 | precision: 0.823 | recall: 0.785 | accuracy: 0.836 | valid_loss: 0.397 | train_loss: 0.443 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 500/1500 [17:12<4:35:53, 16.55s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.791 | precision: 0.887 | recall: 0.713 | accuracy: 0.839 | valid_loss: 0.386 | train_loss: 0.386 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 600/1500 [20:37<4:07:32, 16.50s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.794 | precision: 0.777 | recall: 0.810 | accuracy: 0.820 | valid_loss: 0.412 | train_loss: 0.391 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 700/1500 [24:01<3:40:12, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.790 | precision: 0.794 | recall: 0.785 | accuracy: 0.822 | valid_loss: 0.430 | train_loss: 0.366 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 800/1500 [27:25<3:12:42, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.780 | precision: 0.901 | recall: 0.688 | accuracy: 0.835 | valid_loss: 0.380 | train_loss: 0.384 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 900/1500 [30:50<2:44:50, 16.48s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.775 | precision: 0.900 | recall: 0.680 | accuracy: 0.832 | valid_loss: 0.401 | train_loss: 0.312 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 1000/1500 [34:15<2:17:56, 16.55s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.788 | precision: 0.848 | recall: 0.735 | accuracy: 0.831 | valid_loss: 0.416 | train_loss: 0.305 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 1100/1500 [37:39<1:50:24, 16.56s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.773 | precision: 0.888 | recall: 0.685 | accuracy: 0.829 | valid_loss: 0.399 | train_loss: 0.334 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1200/1500 [41:04<1:22:55, 16.59s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.785 | precision: 0.873 | recall: 0.713 | accuracy: 0.834 | valid_loss: 0.414 | train_loss: 0.271 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 1300/1500 [44:28<55:06, 16.53s/it]  ","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.777 | precision: 0.804 | recall: 0.752 | accuracy: 0.816 | valid_loss: 0.475 | train_loss: 0.255 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 1400/1500 [47:52<27:32, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.794 | precision: 0.833 | recall: 0.759 | accuracy: 0.832 | valid_loss: 0.435 | train_loss: 0.285 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [51:16<00:00,  2.05s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.794 | precision: 0.850 | recall: 0.745 | accuracy: 0.835 | valid_loss: 0.418 | train_loss: 0.298 | best_score: 0.804\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"FOLD 2\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████| 5988/5988 [00:06<00:00, 867.99it/s]\n100%|████████████████████████████| 1497/1497 [00:01<00:00, 938.40it/s]\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.000 | precision: 0.000 | recall: 0.000 | accuracy: 0.574 | valid_loss: 0.699 ","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 100/1500 [03:28<6:46:52, 17.44s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.689 | precision: 0.682 | recall: 0.696 | accuracy: 0.732 | valid_loss: 0.550 | train_loss: 0.668 | best_score: 0.689\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 200/1500 [06:56<6:15:40, 17.34s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.767 | precision: 0.790 | recall: 0.745 | accuracy: 0.807 | valid_loss: 0.464 | train_loss: 0.480 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 300/1500 [10:21<5:30:27, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.752 | precision: 0.820 | recall: 0.694 | accuracy: 0.805 | valid_loss: 0.479 | train_loss: 0.399 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 400/1500 [13:45<5:02:54, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.748 | precision: 0.827 | recall: 0.683 | accuracy: 0.804 | valid_loss: 0.469 | train_loss: 0.380 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 500/1500 [17:10<4:35:16, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.738 | precision: 0.922 | recall: 0.614 | accuracy: 0.814 | valid_loss: 0.459 | train_loss: 0.379 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 600/1500 [20:35<4:08:00, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.754 | precision: 0.727 | recall: 0.782 | accuracy: 0.782 | valid_loss: 0.555 | train_loss: 0.348 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 700/1500 [24:00<3:40:20, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.753 | precision: 0.766 | recall: 0.740 | accuracy: 0.793 | valid_loss: 0.536 | train_loss: 0.347 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 800/1500 [27:24<3:12:51, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.745 | precision: 0.754 | recall: 0.737 | accuracy: 0.786 | valid_loss: 0.552 | train_loss: 0.314 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 900/1500 [30:49<2:45:27, 16.55s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.735 | precision: 0.756 | recall: 0.715 | accuracy: 0.780 | valid_loss: 0.591 | train_loss: 0.281 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 1000/1500 [34:14<2:17:44, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.754 | precision: 0.839 | recall: 0.685 | accuracy: 0.810 | valid_loss: 0.483 | train_loss: 0.309 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 1100/1500 [37:39<1:50:14, 16.54s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.742 | precision: 0.746 | recall: 0.738 | accuracy: 0.782 | valid_loss: 0.555 | train_loss: 0.288 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1200/1500 [41:03<1:23:16, 16.66s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.740 | precision: 0.762 | recall: 0.719 | accuracy: 0.785 | valid_loss: 0.600 | train_loss: 0.259 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 1300/1500 [44:28<55:13, 16.57s/it]  ","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.730 | precision: 0.782 | recall: 0.685 | accuracy: 0.784 | valid_loss: 0.637 | train_loss: 0.222 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 1400/1500 [47:52<27:36, 16.57s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.718 | precision: 0.774 | recall: 0.669 | accuracy: 0.776 | valid_loss: 0.674 | train_loss: 0.237 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [51:16<00:00,  2.05s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.719 | precision: 0.698 | recall: 0.741 | accuracy: 0.753 | valid_loss: 0.671 | train_loss: 0.249 | best_score: 0.767\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"FOLD 3\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████| 5988/5988 [00:06<00:00, 925.69it/s]\n100%|████████████████████████████| 1497/1497 [00:01<00:00, 924.53it/s]\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.275 | precision: 0.480 | recall: 0.193 | accuracy: 0.567 | valid_loss: 0.690 ","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 100/1500 [03:30<7:03:06, 18.13s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.631 | precision: 0.754 | recall: 0.542 | accuracy: 0.729 | valid_loss: 0.544 | train_loss: 0.659 | best_score: 0.631\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 200/1500 [06:57<6:17:48, 17.44s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.763 | precision: 0.816 | recall: 0.716 | accuracy: 0.810 | valid_loss: 0.459 | train_loss: 0.488 | best_score: 0.763\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 300/1500 [10:26<5:50:15, 17.51s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.772 | precision: 0.832 | recall: 0.721 | accuracy: 0.819 | valid_loss: 0.451 | train_loss: 0.414 | best_score: 0.772\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 400/1500 [13:56<5:34:22, 18.24s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.776 | precision: 0.785 | recall: 0.766 | accuracy: 0.811 | valid_loss: 0.441 | train_loss: 0.380 | best_score: 0.776\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 500/1500 [17:23<4:48:44, 17.32s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.782 | precision: 0.848 | recall: 0.726 | accuracy: 0.828 | valid_loss: 0.425 | train_loss: 0.363 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 600/1500 [20:48<4:07:47, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.776 | precision: 0.787 | recall: 0.765 | accuracy: 0.812 | valid_loss: 0.477 | train_loss: 0.357 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 700/1500 [24:13<3:40:18, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.782 | precision: 0.841 | recall: 0.730 | accuracy: 0.826 | valid_loss: 0.424 | train_loss: 0.352 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 800/1500 [27:37<3:12:34, 16.51s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.773 | precision: 0.759 | recall: 0.787 | accuracy: 0.803 | valid_loss: 0.495 | train_loss: 0.325 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 900/1500 [31:02<2:45:05, 16.51s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.768 | precision: 0.789 | recall: 0.749 | accuracy: 0.808 | valid_loss: 0.509 | train_loss: 0.315 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 1000/1500 [34:27<2:17:44, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.759 | precision: 0.719 | recall: 0.803 | accuracy: 0.782 | valid_loss: 0.598 | train_loss: 0.296 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 1100/1500 [37:52<1:50:12, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.762 | precision: 0.708 | recall: 0.824 | accuracy: 0.780 | valid_loss: 0.554 | train_loss: 0.319 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1200/1500 [41:16<1:22:35, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.749 | precision: 0.676 | recall: 0.839 | accuracy: 0.760 | valid_loss: 0.679 | train_loss: 0.261 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 1300/1500 [44:41<55:04, 16.52s/it]  ","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.766 | precision: 0.748 | recall: 0.785 | accuracy: 0.796 | valid_loss: 0.610 | train_loss: 0.249 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 1400/1500 [48:06<27:48, 16.69s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.762 | precision: 0.732 | recall: 0.795 | accuracy: 0.788 | valid_loss: 0.604 | train_loss: 0.242 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [51:30<00:00,  2.06s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.736 | precision: 0.674 | recall: 0.812 | accuracy: 0.752 | valid_loss: 0.744 | train_loss: 0.248 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"FOLD 4\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████| 5988/5988 [00:06<00:00, 935.23it/s]\n100%|████████████████████████████| 1497/1497 [00:01<00:00, 956.38it/s]\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.598 | precision: 0.432 | recall: 0.972 | accuracy: 0.444 | valid_loss: 0.696 ","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 100/1500 [03:35<7:38:40, 19.66s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.627 | precision: 0.856 | recall: 0.495 | accuracy: 0.749 | valid_loss: 0.606 | train_loss: 0.675 | best_score: 0.627\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 200/1500 [07:10<7:04:56, 19.61s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.771 | precision: 0.720 | recall: 0.830 | accuracy: 0.790 | valid_loss: 0.487 | train_loss: 0.505 | best_score: 0.771\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 300/1500 [10:35<5:30:23, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.768 | precision: 0.796 | recall: 0.743 | accuracy: 0.810 | valid_loss: 0.459 | train_loss: 0.438 | best_score: 0.771\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 400/1500 [14:09<5:59:10, 19.59s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.772 | precision: 0.795 | recall: 0.750 | accuracy: 0.812 | valid_loss: 0.425 | train_loss: 0.404 | best_score: 0.772\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 500/1500 [17:38<4:51:55, 17.52s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.780 | precision: 0.795 | recall: 0.766 | accuracy: 0.816 | valid_loss: 0.414 | train_loss: 0.382 | best_score: 0.780\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 600/1500 [21:03<4:07:05, 16.47s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.779 | precision: 0.723 | recall: 0.846 | accuracy: 0.796 | valid_loss: 0.490 | train_loss: 0.380 | best_score: 0.780\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 700/1500 [24:28<3:39:34, 16.47s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.777 | precision: 0.800 | recall: 0.755 | accuracy: 0.816 | valid_loss: 0.421 | train_loss: 0.368 | best_score: 0.780\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 800/1500 [28:01<3:47:14, 19.48s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.781 | precision: 0.769 | recall: 0.793 | accuracy: 0.810 | valid_loss: 0.445 | train_loss: 0.312 | best_score: 0.781\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 900/1500 [31:37<3:14:44, 19.47s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.782 | precision: 0.761 | recall: 0.804 | accuracy: 0.809 | valid_loss: 0.439 | train_loss: 0.343 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 1000/1500 [35:02<2:17:32, 16.50s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.766 | precision: 0.768 | recall: 0.765 | accuracy: 0.802 | valid_loss: 0.459 | train_loss: 0.335 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 1100/1500 [38:27<1:50:12, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.762 | precision: 0.891 | recall: 0.666 | accuracy: 0.823 | valid_loss: 0.454 | train_loss: 0.309 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1200/1500 [41:51<1:22:40, 16.54s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.758 | precision: 0.714 | recall: 0.807 | accuracy: 0.780 | valid_loss: 0.498 | train_loss: 0.303 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 1300/1500 [45:16<55:06, 16.53s/it]  ","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.765 | precision: 0.768 | recall: 0.763 | accuracy: 0.801 | valid_loss: 0.487 | train_loss: 0.271 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 1400/1500 [48:41<27:33, 16.54s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.757 | precision: 0.831 | recall: 0.695 | accuracy: 0.810 | valid_loss: 0.515 | train_loss: 0.252 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [52:06<00:00,  2.08s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.760 | precision: 0.761 | recall: 0.760 | accuracy: 0.796 | valid_loss: 0.485 | train_loss: 0.274 | best_score: 0.782\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"FOLD 5\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████| 5988/5988 [00:06<00:00, 903.82it/s]\n100%|████████████████████████████| 1497/1497 [00:01<00:00, 924.17it/s]\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.597 | precision: 0.426 | recall: 1.000 | accuracy: 0.426 | valid_loss: 0.708 ","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 100/1500 [03:35<7:37:14, 19.60s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.740 | precision: 0.843 | recall: 0.659 | accuracy: 0.803 | valid_loss: 0.515 | train_loss: 0.650 | best_score: 0.740\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 200/1500 [07:10<7:02:49, 19.51s/it]","output_type":"stream"},{"name":"stdout","text":"00200 steps | f1: 0.812 | precision: 0.894 | recall: 0.744 | accuracy: 0.854 | valid_loss: 0.398 | train_loss: 0.483 | best_score: 0.812\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 300/1500 [10:44<6:29:21, 19.47s/it]","output_type":"stream"},{"name":"stdout","text":"00300 steps | f1: 0.815 | precision: 0.770 | recall: 0.867 | accuracy: 0.833 | valid_loss: 0.435 | train_loss: 0.430 | best_score: 0.815\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 400/1500 [14:12<5:22:39, 17.60s/it]","output_type":"stream"},{"name":"stdout","text":"00400 steps | f1: 0.822 | precision: 0.816 | recall: 0.829 | accuracy: 0.848 | valid_loss: 0.375 | train_loss: 0.424 | best_score: 0.822\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 500/1500 [17:37<4:35:30, 16.53s/it]","output_type":"stream"},{"name":"stdout","text":"00500 steps | f1: 0.799 | precision: 0.890 | recall: 0.725 | accuracy: 0.845 | valid_loss: 0.371 | train_loss: 0.386 | best_score: 0.822\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 600/1500 [21:02<4:08:07, 16.54s/it]","output_type":"stream"},{"name":"stdout","text":"00600 steps | f1: 0.798 | precision: 0.728 | recall: 0.882 | accuracy: 0.810 | valid_loss: 0.445 | train_loss: 0.378 | best_score: 0.822\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 700/1500 [24:27<3:41:09, 16.59s/it]","output_type":"stream"},{"name":"stdout","text":"00700 steps | f1: 0.821 | precision: 0.854 | recall: 0.790 | accuracy: 0.853 | valid_loss: 0.363 | train_loss: 0.401 | best_score: 0.822\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 800/1500 [28:02<3:48:45, 19.61s/it]","output_type":"stream"},{"name":"stdout","text":"00800 steps | f1: 0.828 | precision: 0.852 | recall: 0.805 | accuracy: 0.858 | valid_loss: 0.357 | train_loss: 0.368 | best_score: 0.828\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 900/1500 [31:27<2:45:28, 16.55s/it]","output_type":"stream"},{"name":"stdout","text":"00900 steps | f1: 0.821 | precision: 0.857 | recall: 0.788 | accuracy: 0.854 | valid_loss: 0.369 | train_loss: 0.332 | best_score: 0.828\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 1000/1500 [34:55<2:25:34, 17.47s/it]","output_type":"stream"},{"name":"stdout","text":"01000 steps | f1: 0.830 | precision: 0.860 | recall: 0.802 | accuracy: 0.860 | valid_loss: 0.370 | train_loss: 0.317 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 1100/1500 [38:20<1:49:39, 16.45s/it]","output_type":"stream"},{"name":"stdout","text":"01100 steps | f1: 0.817 | precision: 0.784 | recall: 0.854 | accuracy: 0.838 | valid_loss: 0.404 | train_loss: 0.360 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 1200/1500 [41:44<1:22:34, 16.52s/it]","output_type":"stream"},{"name":"stdout","text":"01200 steps | f1: 0.795 | precision: 0.715 | recall: 0.896 | accuracy: 0.804 | valid_loss: 0.563 | train_loss: 0.280 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 1300/1500 [45:09<55:02, 16.51s/it]  ","output_type":"stream"},{"name":"stdout","text":"01300 steps | f1: 0.817 | precision: 0.811 | recall: 0.823 | accuracy: 0.843 | valid_loss: 0.403 | train_loss: 0.311 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 1400/1500 [48:35<27:36, 16.56s/it]","output_type":"stream"},{"name":"stdout","text":"01400 steps | f1: 0.814 | precision: 0.821 | recall: 0.807 | accuracy: 0.843 | valid_loss: 0.406 | train_loss: 0.306 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [51:59<00:00,  2.08s/it]","output_type":"stream"},{"name":"stdout","text":"01500 steps | f1: 0.805 | precision: 0.747 | recall: 0.873 | accuracy: 0.820 | valid_loss: 0.504 | train_loss: 0.294 | best_score: 0.830\n","output_type":"stream"},{"name":"stderr","text":"\n100%|███████████████████████████| 3263/3263 [00:03<00:00, 1068.31it/s]\n100%|██████████| 3263/3263 [04:26<00:00, 12.23it/s]\n100%|██████████| 3263/3263 [04:27<00:00, 12.20it/s]\n100%|██████████| 3263/3263 [04:31<00:00, 12.01it/s]\n100%|██████████| 3263/3263 [04:31<00:00, 12.01it/s]\n100%|██████████| 3263/3263 [04:31<00:00, 12.03it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# XLNet Model","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def train_valid_split(self, train_fraction=.8, shuffle=True):\n        num_train_examples = int(len(self) * train_fraction)\n        train_dataset = copy.deepcopy(self)\n        \n        if shuffle:\n            random.shuffle(train_dataset.data)\n        \n        valid_dataset = copy.deepcopy(train_dataset)\n        train_dataset.data = train_dataset.data[:num_train_examples]\n        valid_dataset.data = valid_dataset.data[num_train_examples:]\n        \n        return train_dataset, valid_dataset","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n        \n        self.model = AutoModel.from_pretrained(model)\n        hidden_size = self.model.config.hidden_size\n        \n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, classes) # projection\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        res = self.model.forward(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n        res = res[0]\n        res = res[:,0,:] # encoding for <s> token\n        res = self.project(res)\n        return res\n    \n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          accelerator,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_best_XLNet.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n                y_true = accelerator.prepare(y_true)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        y_true = accelerator.prepare(y_true)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n        loss = loss_fn(y_hat, hots)\n        \n        # backprop\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k += 1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        state_dict = torch.load(filepath)\n\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if \"module.\" not in k:\n                name = 'module.' + k\n                new_state_dict[name] = v\n\n        model.load_state_dict(new_state_dict)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\nmodel_checkpoint = \"xlnet-large-cased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Create an instance of your custom Dataset class\ndataset = MyDataset(filtered_df, 'final_text', tokenizer)\ntrain_dataset, valid_dataset = dataset.train_valid_split()\n\ndataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)\n\n# Create a DataLoader\nbatch_size = 12\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\nmodel = FinetuneClassifier(head_dropout=.1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \nlogs_dict = {}","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:25:13.246409Z","iopub.execute_input":"2023-05-22T22:25:13.246869Z","iopub.status.idle":"2023-05-22T22:31:27.925354Z","shell.execute_reply.started":"2023-05-22T22:25:13.246830Z","shell.execute_reply":"2023-05-22T22:31:27.924352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train(\n  model, \n  train_dataloader, \n  valid_dataloader, \n  2000, \n  optimizer, \n  accelerator,\n  blind_steps=100, \n  additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n  losses_dict=logs_dict,\n  scheduler=scheduler\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:25:13.246409Z","iopub.execute_input":"2023-05-22T22:25:13.246869Z","iopub.status.idle":"2023-05-22T22:31:27.925354Z","shell.execute_reply.started":"2023-05-22T22:25:13.246830Z","shell.execute_reply":"2023-05-22T22:31:27.924352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86f4a32687c425e9858a8d2c8f91543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"748313644f974b34b880f78b02fd79c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bbcbcd956bf4b929b39c87966139ddc"}},"metadata":{}},{"name":"stderr","text":"100%|████████████████████████████| 7485/7485 [00:17<00:00, 418.90it/s]\n100%|████████████████████████████| 3263/3263 [00:03<00:00, 967.56it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dd2d15d15cb4307adbae41aa66562e9"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"00000 steps | f1: 0.397 | precision: 0.437 | recall: 0.363 | accuracy: 0.524 | valid_loss: 0.872 ","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [03:58<00:00,  2.38s/it]","output_type":"stream"},{"name":"stdout","text":"00100 steps | f1: 0.312 | precision: 0.608 | recall: 0.210 | accuracy: 0.602 | valid_loss: 0.651 | train_loss: 0.870 | best_score: 0.397\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ELECTRA Model","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, target='target', max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append(((tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device)), torch.tensor(row[target]).to(device)))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n    \n    def train_valid_split(self, train_fraction=.8, shuffle=True):\n        num_train_examples = int(len(self) * train_fraction)\n        train_dataset = copy.deepcopy(self)\n        \n        if shuffle:\n            random.shuffle(train_dataset.data)\n        \n        valid_dataset = copy.deepcopy(train_dataset)\n        train_dataset.data = train_dataset.data[:num_train_examples]\n        valid_dataset.data = valid_dataset.data[num_train_examples:]\n        \n        return train_dataset, valid_dataset","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDatasetTest(Dataset):\n    def __init__(self, dataframe, text_column, tokenizer, max_length = 256):\n        self.data = []\n        \n        for index, row in tqdm(dataframe.iterrows(), total=len(dataframe), ncols=70):\n            text = row[text_column]\n            tokenized = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors='pt')\n            self.data.append((torch.tensor(row['id']).to(device) ,(tokenized['input_ids'][0].to(device), tokenized['attention_mask'][0].to(device))))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FinetuneClassifier(nn.Module):\n    def __init__(self, model=model_checkpoint, classes=2, head_dropout=0.2):\n        super().__init__()\n\n        self.model = ElectraModel.from_pretrained(model)\n        hidden_size = self.model.config.hidden_size\n\n        self.project = torch.nn.Sequential(\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, hidden_size),\n            torch.nn.Dropout(head_dropout),\n            torch.nn.Linear(hidden_size, classes) \n        )\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        last_hidden_state = outputs[0]\n        cls_output = last_hidden_state[:, 0, :]\n        cls_output = self.project(cls_output)\n        return cls_output\n\n    def parameters_num(self):\n        return sum(p.numel() for p in self.parameters())","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, \n          train_dataloader, \n          valid_dataloader, \n          steps, \n          optimizer,\n          accelerator,\n          blind_steps=None,\n          loss_fn=torch.nn.BCELoss(),\n          main_metric=('f1', f1_score), \n          additional_metrics=[],\n          filepath='model_best_ELECTRA.pt',\n          load_best=True,\n          scheduler=None,\n          losses_dict=None):\n    \n    if blind_steps == None:\n        blind_steps = len(train_dataloader) // 4\n    \n    def evaluate():  # the first score returned is the main\n        model.eval()\n        \n        y_trues = []\n        y_hats = []\n        \n        loss = 0\n        k = 0\n        \n        with torch.no_grad():\n            for batch in valid_dataloader:\n                \n                (ids, mask), y_true = batch\n                ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n                y_true = accelerator.prepare(y_true)\n                hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n                y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n                loss += float(loss_fn(y_hat, hots))\n                k += 1\n                \n                for i in range(y_true.shape[0]):\n                    y_trues.append(int(y_true[i]))\n                    y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n        \n        scores = [(main_metric[0], main_metric[1](y_trues, y_hats))]\n        \n        for metric in additional_metrics:\n            scores.append((metric[0], metric[1](y_trues, y_hats)))        \n        \n        model.train()\n        return scores + [('valid_loss', loss/k)]\n    \n    \n    def render_scores(scores, step, best=None):\n        print('{:05d} steps'.format(step), end=' ')\n        \n        for score in scores:\n            print(\"| {}: {:.3f}\".format(*score), end=' ')\n            \n        if best != None:\n            print('| best_score: {:.3f}'.format(best))\n            \n    \n    # initial scores\n    scores = evaluate()\n    render_scores(scores, 0)\n    best_score = scores[0][1]\n    torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n    \n    # logs\n    if losses_dict != None:\n        losses_dict['train_loss'] = []\n        losses_dict['valid_loss'] = []\n        losses_dict[main_metric[0]] = []\n    \n    epoch_loss = 0\n    k = 0\n    \n    train_iter = iter(train_dataloader)\n    model.train()\n    \n    for step in tqdm(range(steps)):\n        \n        # retrieving a batch\n        try:\n            batch = next(train_iter)\n        except:\n            train_iter = iter(train_dataloader)\n            batch = next(train_iter)\n\n        (ids, mask), y_true = batch\n        ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n        y_true = accelerator.prepare(y_true)\n\n        # prediction\n        y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n        hots = torch.nn.functional.one_hot(y_true, 2).to(dtype=torch.float)\n        loss = loss_fn(y_hat, hots)\n        \n        # backprop\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        \n        if scheduler != None:\n            scheduler.step()\n            \n        epoch_loss += float(loss)\n        k += 1\n        \n        # evaluation\n        if (step + 1) % blind_steps == 0:\n            scores = evaluate() + [('train_loss', epoch_loss/k)]\n            \n            if losses_dict != None:\n                losses_dict['valid_loss'].append(float(scores[-2][1]))\n                losses_dict['train_loss'].append(float(scores[-1][1]))\n                losses_dict[main_metric[0]].append(float(scores[0][1]))\n            \n            if scores[0][1] > best_score:\n                best_score = scores[0][1]\n                torch.save(accelerator.unwrap_model(model).state_dict(), filepath)\n                \n            render_scores(scores, step + 1, best=best_score)\n            epoch_loss = 0\n            k = 0\n                \n    if load_best:\n        state_dict = torch.load(filepath)\n\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            if \"module.\" not in k:\n                name = 'module.' + k\n                new_state_dict[name] = v\n\n        model.load_state_dict(new_state_dict)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = accelerator.device\nmodel_checkpoint = 'google/electra-base-discriminator'\n\ntokenizer = ElectraTokenizer.from_pretrained(model_checkpoint)\nmodel = ElectraForSequenceClassification.from_pretrained(model_checkpoint)\n\n# Create an instance of your custom Dataset class\ndataset = MyDataset(filtered_df, 'final_text', tokenizer)\ntrain_dataset, valid_dataset = dataset.train_valid_split()\n\ndataset_test = MyDatasetTest(filtered_test, 'final_text', tokenizer)\n\n# Create a DataLoader\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\nmodel = FinetuneClassifier(head_dropout=.1)\nmodel = nn.DataParallel(model)\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=500) \nlogs_dict = {}","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(\n  model, \n  train_dataloader, \n  valid_dataloader, \n  2000, \n  optimizer, \n  accelerator,\n  blind_steps=100, \n  additional_metrics=[('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)],\n  losses_dict=logs_dict,\n  scheduler=scheduler\n)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting","metadata":{}},{"cell_type":"code","source":"model_name = 'BERT'","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_dict = torch.load(f'/kaggle/working/model_best_{model_name}.pt')\n\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    if 'module.' not in k:\n        name = 'module.' + k\n        new_state_dict[name] = v\n    \nmodel.load_state_dict(new_state_dict)\n\ndef evaluate(model, valid_dataloader, metrics=[('f1', f1_score),('precision', precision_score), ('recall', recall_score),('accuracy', accuracy_score)]):\n    model.eval()\n\n    y_trues = []\n    y_hats = []\n\n    with torch.no_grad():\n        for batch in valid_dataloader:\n\n            (ids, mask), y_true = batch\n            y_hat = torch.softmax(model.forward(input_ids=ids, attention_mask=mask),dim=-1)\n\n            for i in range(y_true.shape[0]):\n                y_trues.append(int(y_true[i]))\n                y_hats.append(1 if y_hat[i][0] < y_hat[i][1] else 0)\n\n    scores = []\n\n    for metric in metrics:\n        scores.append((metric[0], metric[1](y_trues, y_hats)))        \n \n    return scores\n\nscores = evaluate(model, valid_dataloader)\nprint(scores)\n\npredictions_df = pd.DataFrame()\nfor i, (ids, mask) in tqdm(dataset_test):\n    ids, mask = accelerator.prepare(ids), accelerator.prepare(mask)\n    pred = model(input_ids=ids[None], attention_mask=mask[None])[0]\n    y_hat = 1 if pred[0] < pred[1] else 0\n    r = [int(i), y_hat]\n    predictions_df = pd.concat([predictions_df, pd.DataFrame(np.array(r)[None,:], columns=['id', 'target'])])\n\npredictions_df.to_csv('submission.csv', index=False)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}