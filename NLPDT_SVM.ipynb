{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Importing"
      ],
      "metadata": {
        "id": "2b6aw69TWwIQ"
      },
      "id": "2b6aw69TWwIQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoJx37nXXfBq",
        "outputId": "0ad87bed-3dcb-4b15-f2db-24ae157f8fb2"
      },
      "id": "AoJx37nXXfBq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1fb60d34",
      "metadata": {
        "id": "1fb60d34"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/NLPDT/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLPDT/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Importing"
      ],
      "metadata": {
        "id": "VYDB0SHdWzdI"
      },
      "id": "VYDB0SHdWzdI"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f6c1ecd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6c1ecd2",
        "outputId": "1d1c2336-b134-43a9-cc8b-7fc1774a897d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import emoji\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "e27euYtHW4Wd"
      },
      "id": "e27euYtHW4Wd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing with Baseline Method"
      ],
      "metadata": {
        "id": "dOGfLJ62W-vf"
      },
      "id": "dOGfLJ62W-vf"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9ff4a8e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ff4a8e9",
        "outputId": "27ed9179-aa36-479f-8ae4-edec21923dfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7503"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_df = train_df.drop_duplicates('text')\n",
        "len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f250beb3",
      "metadata": {
        "id": "f250beb3"
      },
      "outputs": [],
      "source": [
        "def removeEmoji(text):\n",
        "    return emoji.replace_emoji(text, '')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    text = text.lower() #convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #remove punctuation\n",
        "    text = removeEmoji(text) #remove emoji\n",
        "    tk = WhitespaceTokenizer()  #tokenize text to list of words without space\n",
        "    textsplit = tk.tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tag = nltk.pos_tag(textsplit) #get tags (noun, verb...)\n",
        "\n",
        "    result = []\n",
        "    for i,w in enumerate(tag):\n",
        "        word = w[0]\n",
        "        tag = w[1]\n",
        "        if word not in stopwords.words(\"english\") and not word.startswith(\"http\") and \"\\\\\" not in word and '#' not in word and '@' not in word:\n",
        "            result.append(lemmatizer.lemmatize(word,get_wordnet_pos(tag))) #lemmartize word to current tense\n",
        "\n",
        "    text = ' '.join(result) #join back the result\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38b2df7",
      "metadata": {
        "id": "c38b2df7"
      },
      "source": [
        "### Preprocessing with Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "79bc0d7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79bc0d7c",
        "outputId": "3b51003f-1f96-4b00-fc3b-ee2a734246ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.4237315220\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "t = time.process_time()\n",
        "\n",
        "result = []\n",
        "for index, tweet in enumerate(train_df['text']):\n",
        "    tweet = tweet_cleaner(tweet)\n",
        "    result.append(tweet)\n",
        "\n",
        "elapsed_time = time.process_time() - t\n",
        "print(\"time: %0.10f\" % elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "385ca4aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "385ca4aa",
        "outputId": "da81b7a6-3e5f-4bc4-f472-f49a8bda8048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 16.1167844240\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "t = time.process_time()\n",
        "\n",
        "result = list(map(lambda x: tweet_cleaner(x), train_df['text']))\n",
        "\n",
        "elapsed_time = time.process_time() - t\n",
        "print(\"time: %0.10f\" % elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "07b5e707",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07b5e707",
        "outputId": "c6536f14-4da2-4b30-e2d9-82e35f95172d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.1 s, sys: 1.19 s, total: 15.3 s\n",
            "Wall time: 15.8 s\n"
          ]
        }
      ],
      "source": [
        "%time result = list(map(lambda x: tweet_cleaner(x), train_df['text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3669b351",
      "metadata": {
        "id": "3669b351"
      },
      "source": [
        "### Preprocessing with Decreased Functions Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320723d4",
      "metadata": {
        "id": "320723d4"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    text = text.lower() #convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #remove punctuation\n",
        "    text = emoji.replace_emoji(text, '') #remove emoji\n",
        "    tk = WhitespaceTokenizer()  #tokenize text to list of words without space\n",
        "    textsplit = tk.tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tag = nltk.pos_tag(textsplit) #get tags (noun, verb...)\n",
        "\n",
        "    result = []\n",
        "    for i,w in enumerate(tag):\n",
        "        word = w[0]\n",
        "        tag = w[1]\n",
        "        if word not in stopwords.words(\"english\") and not word.startswith(\"http\") and \"\\\\\" not in word and '#' not in word and '@' not in word:\n",
        "            result.append(lemmatizer.lemmatize(word,get_wordnet_pos(tag))) #lemmartize word to current tense\n",
        "\n",
        "    text = ' '.join(result) #join back the result\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b3cebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69b3cebe",
        "outputId": "480af2bf-1601-4693-a2ed-76b7f74933d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.9 s, sys: 1.74 s, total: 22.6 s\n",
            "Wall time: 22.8 s\n"
          ]
        }
      ],
      "source": [
        "%time result = list(map(lambda x: tweet_cleaner(x), train_df['text']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_cleaner(text):\n",
        "    text = text.lower() #convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #remove punctuation\n",
        "    text = emoji.replace_emoji(text, '') #remove emoji\n",
        "    tk = WhitespaceTokenizer()  #tokenize text to list of words without space\n",
        "    textsplit = tk.tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tag = nltk.pos_tag(textsplit) #get tags (noun, verb...)\n",
        "\n",
        "    result = []\n",
        "    for i,w in enumerate(tag):\n",
        "        word = w[0]\n",
        "        tag = w[1]\n",
        "        if word not in stopwords.words(\"english\") and not word.startswith(\"http\") and \"\\\\\" not in word and '#' not in word and '@' not in word:\n",
        "            if tag.startswith('J'):\n",
        "                tag = wordnet.ADJ\n",
        "            elif tag.startswith('V'):\n",
        "                tag = wordnet.VERB\n",
        "            elif tag.startswith('N'):\n",
        "                tag = wordnet.NOUN\n",
        "            elif tag.startswith('R'):\n",
        "                tag = wordnet.ADV\n",
        "            else:\n",
        "                tag = wordnet.NOUN\n",
        "            result.append(lemmatizer.lemmatize(word, pos = tag)) #lemmartize word to current tense\n",
        "\n",
        "    text = ' '.join(result) #join back the result\n",
        "    return text"
      ],
      "metadata": {
        "id": "hBjEQDLAY0i0"
      },
      "id": "hBjEQDLAY0i0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time result = list(map(lambda x: tweet_cleaner(x), train_df['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOgL_oTeY3jj",
        "outputId": "0ab6f6b8-eb17-48a2-98d0-95c0a8438bdf"
      },
      "id": "NOgL_oTeY3jj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.4 s, sys: 1.65 s, total: 22 s\n",
            "Wall time: 24.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49dd70c9",
      "metadata": {
        "id": "49dd70c9"
      },
      "source": [
        "### Preprocessing with Multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a4daa7",
      "metadata": {
        "id": "44a4daa7"
      },
      "outputs": [],
      "source": [
        "from multiprocessing.pool import Pool\n",
        "\n",
        "def do_work(text):\n",
        "\n",
        "    def get_wordnet_pos(treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN\n",
        "\n",
        "    text = text.lower() #convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #remove punctuation\n",
        "    text = emoji.replace_emoji(text, '') #remove emoji\n",
        "    tk = WhitespaceTokenizer()  #tokenize text to list of words without space\n",
        "    textsplit = tk.tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tag = nltk.pos_tag(textsplit) #get tags (noun, verb...)\n",
        "\n",
        "    res = []\n",
        "    for i,w in enumerate(tag):\n",
        "        word = w[0]\n",
        "        tag = w[1]\n",
        "        if word not in stopwords.words(\"english\") and not word.startswith(\"http\") and \"\\\\\" not in word and '#' not in word and '@' not in word:\n",
        "            res.append(lemmatizer.lemmatize(word,get_wordnet_pos(tag))) #lemmartize word to current tense\n",
        "\n",
        "    text = ' '.join(res) #join back the result\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_text(data, num_processes):\n",
        "\n",
        "    with Pool(num_processes) as p:\n",
        "        results = p.map(do_work, data)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f251ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1f251ba",
        "outputId": "a39e2475-d585-4cc6-bf31-a59d0ea92895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 151 ms, sys: 88 ms, total: 239 ms\n",
            "Wall time: 20.7 s\n"
          ]
        }
      ],
      "source": [
        "%time result = preprocess_text(train_df['text'], 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = result"
      ],
      "metadata": {
        "id": "YaxgVFKgWjuV"
      },
      "id": "YaxgVFKgWjuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_df['text'].tolist(), train_df['target'].tolist(), test_size=0.2, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "9ODryOihVC3L"
      },
      "id": "9ODryOihVC3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Model"
      ],
      "metadata": {
        "id": "YOOIEq3yTYM2"
      },
      "id": "YOOIEq3yTYM2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Train Method"
      ],
      "metadata": {
        "id": "ka-98zekTkJN"
      },
      "id": "ka-98zekTkJN"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "def dotProduct(d1, d2):\n",
        "    result = 0\n",
        "    for f, v in d1.items():\n",
        "        for g, w in d2.items():\n",
        "            if f == g:\n",
        "                result += v * w\n",
        "    return result\n",
        "\n",
        "def increment(d1, scale, d2):\n",
        "    d1_copy = d1.copy()\n",
        "    for f, v in d2.items():\n",
        "        if f in d1_copy:\n",
        "            d1_copy[f] = d1_copy[f] + v * scale\n",
        "        else:\n",
        "            d1_copy[f] = v * scale\n",
        "    return d1_copy\n",
        "\n",
        "def multiplication(scale, d1):\n",
        "    d1_copy = d1.copy()\n",
        "    for f, v in d1_copy.items():\n",
        "        d1_copy[f] = v * scale\n",
        "    return d1_copy\n",
        "\n",
        "def pegasos_advanced(X, y, lbd, max_epochs=15):\n",
        "    s = 1\n",
        "    W = {}\n",
        "    t = 2\n",
        "    for i in tqdm(range(max_epochs)):\n",
        "        for j in range(len(y)):\n",
        "            xj = X[j]\n",
        "            yj = y[j]\n",
        "            eta = 1 / (lbd * t)\n",
        "            margin = s * yj * dotProduct(W, xj)\n",
        "            if margin < 1:\n",
        "                s = (1 - eta * lbd) * s\n",
        "                W = increment(W, eta * yj / s, xj)\n",
        "            else:\n",
        "                s = (1 - eta * lbd) * s\n",
        "            t += 1\n",
        "        norm_w = s ** 2 * dotProduct(W, W)\n",
        "        if norm_w <= 1 / math.sqrt(lbd):\n",
        "            break\n",
        "    W = multiplication(s, W)\n",
        "    return W\n"
      ],
      "metadata": {
        "id": "SU9nbZLo25r-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SU9nbZLo25r-"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "W = pegasos_advanced(X_train, y_train, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RLhqE05259o",
        "outputId": "cd46336e-34a7-4823-91cd-fb8b897cf563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [12:16<00:00, 49.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12min 12s, sys: 1.35 s, total: 12min 14s\n",
            "Wall time: 12min 16s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "id": "7RLhqE05259o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Method with Vectorization"
      ],
      "metadata": {
        "id": "PV6vFgaCTq0K"
      },
      "id": "PV6vFgaCTq0K"
    },
    {
      "cell_type": "code",
      "source": [
        "def dotProduct(d1, d2):\n",
        "    if len(d1) < len(d2):\n",
        "        return dotProduct(d2, d1)\n",
        "    else:\n",
        "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
        "\n",
        "def increment(d1, scale, d2):\n",
        "    for f, v in d2.items():\n",
        "        d1[f] = d1.get(f, 0) + v * scale\n",
        "\n",
        "def multiplication(scale, d1):\n",
        "    for f, v in d1.items():\n",
        "        d1[f] = d1.get(f, 0) * scale\n",
        "\n",
        "def pegasos_advanced(X, y, lbd, max_epochs=15):\n",
        "    y = np.array(y)  # Convert y to a NumPy array\n",
        "    s = 1\n",
        "    W = {}\n",
        "    t = 2\n",
        "    for i in tqdm(range(max_epochs)):\n",
        "        for j in range(len(y)):\n",
        "            xj = X[j]\n",
        "            yj = y[j]\n",
        "            eta = 1 / (lbd * t)\n",
        "            margin = s * yj * dotProduct(W, xj)\n",
        "            if margin < 1:\n",
        "                s = (1 - eta * lbd) * s\n",
        "                increment(W, eta * yj / s, xj)\n",
        "            else:\n",
        "                s = (1 - eta * lbd) * s\n",
        "            t += 1\n",
        "        norm_w = s ** 2 * dotProduct(W, W)\n",
        "        if norm_w <= 1 / math.sqrt(lbd):\n",
        "            break\n",
        "    multiplication(s, W)\n",
        "    return W"
      ],
      "metadata": {
        "id": "COCJSZbqpNMQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "COCJSZbqpNMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "W = pegasos_advanced(X_train, y_train, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3kvvlj1pNTk",
        "outputId": "243fe133-0d90-4e5e-f75e-bca84e5e7779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:01<00:00, 13.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.15 s, sys: 6 ms, total: 1.15 s\n",
            "Wall time: 1.16 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "id": "m3kvvlj1pNTk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Method with Reduced Loop and Vectorizaiton"
      ],
      "metadata": {
        "id": "j-U3hZLgWWrk"
      },
      "id": "j-U3hZLgWWrk"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "def dotProduct(d1, d2):\n",
        "    return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
        "\n",
        "def increment(d1, scale, d2):\n",
        "    for f, v in d2.items():\n",
        "        d1[f] = d1.get(f, 0) + v * scale\n",
        "\n",
        "def multiplication(scale, d1):\n",
        "    for f in d1:\n",
        "        d1[f] *= scale\n",
        "\n",
        "def pegasos_advanced(X, y, lbd, max_epochs=15):\n",
        "    y = np.array(y)  # Convert y to a NumPy array\n",
        "    s = 1\n",
        "    W = {}\n",
        "    t = 2\n",
        "    data = zip(itertools.cycle(X), itertools.cycle(y))\n",
        "    for _ in tqdm(range(max_epochs * len(y))):\n",
        "        xj, yj = next(data)\n",
        "        eta = 1 / (lbd * t)\n",
        "        margin = s * yj * dotProduct(W, xj)\n",
        "        if margin < 1:\n",
        "            s = (1 - eta * lbd) * s\n",
        "            increment(W, eta * yj / s, xj)\n",
        "        else:\n",
        "            s = (1 - eta * lbd) * s\n",
        "        t += 1\n",
        "        if t > max_epochs * len(y):\n",
        "            break\n",
        "    multiplication(s, W)\n",
        "    return W"
      ],
      "metadata": {
        "id": "Mk6OlGT92G4g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Mk6OlGT92G4g"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "W = pegasos_advanced(X_train, y_train, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJVJxMpN2Jj0",
        "outputId": "8c4e1f9c-6ccb-4ceb-cfdc-20f83606f9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 90028/90030 [00:01<00:00, 82221.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.09 s, sys: 8.01 ms, total: 1.1 s\n",
            "Wall time: 1.11 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "id": "MJVJxMpN2Jj0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting"
      ],
      "metadata": {
        "id": "sloN0ozjWdBv"
      },
      "id": "sloN0ozjWdBv"
    },
    {
      "cell_type": "code",
      "source": [
        "def sign(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "\n",
        "def test(W, X_test, y_test):\n",
        "    return sum(1 for x, y_true in zip(X_test, y_test) if y_true == sign(dotProduct(W, x))) / len(y_test)"
      ],
      "metadata": {
        "id": "UfXmHEVXdzgK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UfXmHEVXdzgK"
    },
    {
      "cell_type": "code",
      "source": [
        "test(W, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smcPqApddzmy",
        "outputId": "78c1de49-72d9-4e6c-d3c9-ce2e49299821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.760159893404397"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "id": "smcPqApddzmy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}